{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a416b9e0-5a80-4645-9254-b0cc26685db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16800 images belonging to 2 classes.\n",
      "Found 4200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set the path to your dataset\n",
    "data_dir = r'D:\\Dataset'\n",
    "\n",
    "# Define image size and batch size\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 21\n",
    "\n",
    "# Define data augmentation and normalization\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Load training data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "val_data = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b8621f-6d50-4353-9e8c-1aa55475d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d11f88-3530-4d85-8b90-89e7a9b10ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': 0, 'images_train_test_val': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae499443-f787-44b3-becf-d8c72dc09ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(train_data.class_indices)  # Dynamically set the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed9f469f-bf10-4f21-8de7-7deee124f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',  # This ensures one-hot encoding\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f11e937e-ef14-4e02-af72-9f13ef895b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "# Build the model again\n",
    "model = Sequential([\n",
    "    Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02670e94-adf3-4ce8-a877-5779f7f1588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 2s/step - accuracy: 0.4880 - loss: 0.8014 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 2/2\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m954s\u001b[0m 2s/step - accuracy: 0.5014 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, validation_data=val_data, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfb49c59-3080-47fd-92b2-c12432a3a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    \"agricultural\": (255, 0, 0),        # Red\n",
    "    \"airplane\": (0, 255, 0),            # Green\n",
    "    \"baseballdiamond\": (0, 0, 255),     # Blue\n",
    "    \"beach\": (255, 255, 0),             # Yellow\n",
    "    \"buildings\": (255, 0, 255),         # Magenta\n",
    "    \"chaparral\": (0, 255, 255),         # Cyan\n",
    "    \"denseresidential\": (128, 0, 0),    # Dark Red\n",
    "    \"forest\": (0, 128, 0),              # Dark Green\n",
    "    \"freeway\": (0, 0, 128),             # Dark Blue\n",
    "    \"golfcourse\": (128, 128, 0),        # Olive\n",
    "    \"intersection\": (128, 0, 128),      # Purple\n",
    "    \"mediumresidential\": (0, 128, 128), # Teal\n",
    "    \"mobilehomepark\": (192, 192, 192),  # Silver\n",
    "    \"overpass\": (128, 128, 128),        # Gray\n",
    "    \"parkinglot\": (255, 165, 0),        # Orange\n",
    "    \"river\": (70, 130, 180),            # Steel Blue\n",
    "    \"runway\": (0, 255, 127),            # Spring Green\n",
    "    \"sparseresidential\": (255, 20, 147),# Deep Pink\n",
    "    \"storagetanks\": (75, 0, 130),       # Indigo\n",
    "    \"tenniscourt\": (244, 164, 96),      # Sandy Brown\n",
    "    \"harbor\": (0, 255, 255)             # Cyan\n",
    "}\n",
    "\n",
    "# Function to convert the class output to color\n",
    "def class_to_color(class_idx):\n",
    "    for key, value in color_map.items():\n",
    "        if train_data.class_indices[key] == class_idx:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10738f7-df7b-4d0b-a1f5-96cc2258d837",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 256, 256, 4)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 256, 256, 4), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Test the function with a random image\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mpredict_and_visualize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../EvaluationMaps/1.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mpredict_and_visualize\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Predict the class\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Generate the color-coded map\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 256, 256, 4)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 256, 256, 4), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def predict_and_visualize(image_path):\n",
    "    # Load and preprocess the input image\n",
    "    img = Image.open(image_path).resize((IMG_SIZE, IMG_SIZE))\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Predict the class\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "\n",
    "    # Generate the color-coded map\n",
    "    color = class_to_color(predicted_class)\n",
    "    \n",
    "    # Create a color-coded image\n",
    "    color_image = np.full((IMG_SIZE, IMG_SIZE, 3), color, dtype=np.uint8)\n",
    "\n",
    "    # Show the original and color-coded images\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Color-Coded Map')\n",
    "    plt.imshow(color_image)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Test the function with a random image\n",
    "predict_and_visualize(r\"../EvaluationMaps/1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd7df8-5647-4e99-9044-a79da5591c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
